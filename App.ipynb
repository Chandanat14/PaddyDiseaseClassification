import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import os
from google.colab import drive
import numpy as np
import cv2
# Mount Google Drive
drive.mount('/content/drive')
class_dirs = ["Bacterial Blight", "Downy Mildew", "Blast", "Dead Heart", "Healthy"]
# Data directory where the images are stored in Google Drive
image_dir = '/content/drive/My Drive/dataset'

# Function to preprocess images by converting them to HSV
def preprocess_to_hsv(image):
    hsv_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
    lower_bound = np.array([25, 40, 40])
    upper_bound = np.array([85, 255, 255])
    mask = cv2.inRange(hsv_image, lower_bound, upper_bound)
    masked_image = cv2.bitwise_and(hsv_image, hsv_image, mask=mask)

    return hsv_image

# Custom ImageDataGenerator with the HSV preprocessing
def hsv_image_generator(generator, directory, subset):
    data_gen = generator.flow_from_directory(
        directory,
        target_size=(256, 256),
        batch_size=32,
        class_mode='categorical',
        subset=subset
    )

    while True:
        batch_x, batch_y = next(data_gen)
        batch_x_hsv = np.array([preprocess_to_hsv(img) for img in batch_x])
        yield batch_x_hsv, batch_y

# ImageDataGenerator for training with data augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    vertical_flip=True,
    validation_split=0.2
)

# Create generators for training and validation
train_generator = hsv_image_generator(train_datagen, image_dir, subset='training')

validation_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)
validation_generator = hsv_image_generator(validation_datagen, image_dir, subset='validation')

# Function to count the number of images in each subset
# Function to count the number of images in the dataset
def count_images(directory):
    total = 0
    for class_dir in os.listdir(directory):
        class_path = os.path.join(directory, class_dir)
        if os.path.isdir(class_path):  # Ensure it's a folder
            total += len(os.listdir(class_path))  # Count images in the folder
    return total

# Manually count the number of images in the dataset
train_image_count = count_images(image_dir)  # No 'training' subfolder
validation_image_count = int(train_image_count * 0.2)  # 20% validation split

# Batch size used for training
batch_size = 32

# Calculate steps per epoch and validation steps
steps_per_epoch = int(np.ceil(train_image_count / batch_size))
validation_steps = int(np.ceil(validation_image_count / batch_size))


# Define the CNN model with batch normalization and dropout for regularization
def build_enhanced_cnn_model(input_shape=(256, 256, 3), num_classes=5):
    model = models.Sequential()

    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling2D((2, 2)))

    model.add(layers.Conv2D(64, (3, 3), activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling2D((2, 2)))

    model.add(layers.Conv2D(128, (3, 3), activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling2D((2, 2)))

    model.add(layers.Conv2D(256, (3, 3), activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling2D((2, 2)))

    model.add(layers.Flatten())
    model.add(layers.Dense(256, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.Dropout(0.5))

    model.add(layers.Dense(num_classes, activation='softmax'))

    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    return model

# Build the enhanced CNN model
cnn_model = build_enhanced_cnn_model(input_shape=(256, 256, 3), num_classes=len(class_dirs))

# Print model summary
cnn_model.summary()

# Define callbacks for learning rate adjustment and early stopping
early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
learning_rate_scheduler = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)

# Train the model using the generators
history = cnn_model.fit(
    train_generator,
    validation_data=validation_generator,
    epochs=20,
    steps_per_epoch=steps_per_epoch,
    validation_steps=validation_steps,
    callbacks=[early_stopping, learning_rate_scheduler]
)

# Save the model
cnn_model.save("rice_leaf_disease_classifier_hsv_refactored1.h5")

# Evaluate the model on the validation data
val_loss, val_acc = cnn_model.evaluate(validation_generator)
print(f"Validation Loss: {val_loss}, Validation Accuracy: {val_acc}")
